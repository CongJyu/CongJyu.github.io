<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-10-18T00:54:19+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rain Chen&apos;s Blog</title><subtitle>Rain Chen 嘅個人站點｜RSS 可用</subtitle><author><name>{&quot;name&quot; =&gt; nil, &quot;job_title&quot; =&gt; nil, &quot;location&quot; =&gt; nil, &quot;email&quot; =&gt; nil, &quot;social_links&quot; =&gt; [{&quot;name&quot; =&gt; &quot;github&quot;, &quot;url&quot; =&gt; &quot;https://github.com/congjyu&quot;}, {&quot;name&quot; =&gt; &quot;rss&quot;, &quot;url&quot; =&gt; &quot;https://congjyu.github.io/feed&quot;}]}</name></author><entry><title type="html">Explainable AI - Notes and Practice</title><link href="http://localhost:4000/blog/2025/10/17/XAI/" rel="alternate" type="text/html" title="Explainable AI - Notes and Practice" /><published>2025-10-17T00:00:00+08:00</published><updated>2025-10-17T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/17/XAI</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/17/XAI/">&lt;h1 id=&quot;heading-explainable-ai---notes-and-practice&quot;&gt;Explainable AI - Notes and Practice&lt;/h1&gt;

&lt;h2 id=&quot;heading-linear-regression-and-logistic-regression&quot;&gt;Linear Regression and Logistic Regression&lt;/h2&gt;

&lt;p&gt;We make regression because we want to make a &quot;supervise learning&quot;, that is, input observation $\mathbf{x}$ which is typically a vector in $\mathbb{R}^d$ and output a $y \in \mathbb{R}$ which is a real number. Our goal is to predict the output $y$ from input $\mathbf{x}$. That is, learn the function:&lt;/p&gt;

\[y = f(\mathbf{x}) + \epsilon\]

&lt;p&gt;where $\epsilon$ is the noise.&lt;/p&gt;

&lt;p&gt;Then we come to the linear regression.&lt;/p&gt;

&lt;h3 id=&quot;heading-linear-regression&quot;&gt;Linear Regression&lt;/h3&gt;

&lt;p&gt;Given a linear function of the input feature $x$, $f(x) = \omega * x + b$, where $\omega$ is the slope and $b$ is the intercept. Then we observe a noisy output of the function, then $f(x) = \omega * x + b + \epsilon$ where $\epsilon$ is a assumed Gaussian noise.&lt;/p&gt;

&lt;p&gt;We make it to the d-dimension case. The linear combination (also called weighted sum) of $d$ input variables $x_1, …, x_d$. Now we have:&lt;/p&gt;

\[f(x) = \omega_0 + \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d\]

&lt;p&gt;where $x_j$ is the input features, $\omega_j$ is the input feature weights and $\omega_0$ is the intercept.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;KEY ASSUMPTIONS&lt;/strong&gt; of Linear Regression:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: Prediction is a linear combination of features. (Obviously, linear effects are easily interpretable; and additivity seperates the effects.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Normality&lt;/strong&gt;: Errors are normally distributed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Homoscedasticity&lt;/strong&gt;: Constant variance of errors across feature space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: Each instance is independent. (We do not measure a same thing repeatly.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fixed Features&lt;/strong&gt;: Features are treated as constants, free of measurement error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No Multicollinearity&lt;/strong&gt;: Features should not be strongly correlated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-ordinary-least-squares-ols&quot;&gt;Ordinary Least Squares (OLS)&lt;/h3&gt;

&lt;p&gt;Suppose we have a linear function:&lt;/p&gt;

\[f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b\]

&lt;p&gt;need to estimate the parameter $(\mathbf{w}, b)$ from the data. A method is to fit the parameters by minimizing the squared prediction error on the training set ${\lbrace(\mathbf{x}&lt;em&gt;i, y_i)\rbrace}^N&lt;/em&gt;{i=1}$. Like this:&lt;/p&gt;

\[\min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - f(x_i))^2}} = \min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - (\mathbf{w}^T \mathbf{x}_i + b))^2}}\]

&lt;p&gt;We have a &lt;strong&gt;closed-form solution&lt;/strong&gt; for this. The bias $b$ can be absorbed into $\mathbf{w}$ by redefining:&lt;/p&gt;

\[\mathbf{w} \leftarrow \begin{bmatrix}\mathbf{w}\\b\end{bmatrix}, \mathbf{x} \leftarrow \begin{bmatrix}\mathbf{x}\\1\end{bmatrix}\]

&lt;p&gt;then the minimization problem will be like this:&lt;/p&gt;

\[\min_{\mathbf{w}} ||\mathbf{y} - \mathbf{X}^T \mathbf{w}||^2\]

&lt;p&gt;where $\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, …, \mathbf{x_N}]$ is the data matrix and $\mathbf{y} = [y_1, y_2, …, y_N]^T$ is the vector of outputs.&lt;/p&gt;

&lt;p&gt;So we get the closed-form solution.&lt;/p&gt;

\[\mathbf{w}^* = (\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}\mathbf{y}\]

&lt;p&gt;The $(\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}$ part is also called the pseudo-inverse of $\mathbf{X}$.&lt;/p&gt;</content><author><name>Rain</name></author><category term="CityU" /><category term="Artificial Intelligence" /><category term="XAI" /><summary type="html">Explainable AI - Notes and Practice</summary></entry></feed>