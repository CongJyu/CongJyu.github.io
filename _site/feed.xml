<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-02T15:59:53+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rain Chen&apos;s Blog</title><subtitle>Rain ÂòÖÂÄã‰∫∫Á´ôÈªû</subtitle><author><name>{&quot;name&quot; =&gt; nil, &quot;job_title&quot; =&gt; nil, &quot;location&quot; =&gt; nil, &quot;email&quot; =&gt; nil, &quot;social_links&quot; =&gt; [{&quot;name&quot; =&gt; &quot;github&quot;, &quot;url&quot; =&gt; &quot;https://github.com/congjyu&quot;}, {&quot;name&quot; =&gt; &quot;rss&quot;, &quot;url&quot; =&gt; &quot;https://congjyu.github.io/feed&quot;}]}</name></author><entry><title type="html">Intelligent System - Notes and Practice</title><link href="http://localhost:4000/blog/2025/10/30/IntelligentSystem/" rel="alternate" type="text/html" title="Intelligent System - Notes and Practice" /><published>2025-10-30T00:00:00+08:00</published><updated>2025-10-30T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/30/IntelligentSystem</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/30/IntelligentSystem/">&lt;h1 id=&quot;heading-intelligent-system---notes-and-practice&quot;&gt;Intelligent System - Notes and Practice&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Intelligence is a mental quality that consists of the abilities to learn from experience, adapt to new situations, understanding and handle abstract concepts, and use knowledge to manipulate one&apos;s environment.&lt;/p&gt;

  &lt;p&gt;‚Äì Britannica&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;heading-a-definition-of-intelligent-systems&quot;&gt;A Definition of Intelligent Systems&lt;/h3&gt;

&lt;p&gt;A system is an intelligent system if the system exhibits some intelligent behaviors. (e.g. fuzzy systems, simulated annealing, genetic algorithms and expert systems.)&lt;/p&gt;

&lt;h2 id=&quot;heading-support-vector-machines-svm&quot;&gt;Support Vector Machines (SVM)&lt;/h2&gt;

&lt;h2 id=&quot;heading-„Å§„Å•„Åè&quot;&gt;„Å§„Å•„Åè‚Ä¶&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Based on the provided citations (CS5486 slides 1‚Äì220), here‚Äôs a concise &lt;strong&gt;review note&lt;/strong&gt; to help you prepare for your exam. The content spans foundational topics in machine learning, neural networks, and optimization ‚Äî especially SVMs, perceptrons, ADALINE, and evolutionary algorithms.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;heading--exam-review-note-cs5486-first-220-pages&quot;&gt;üìò &lt;strong&gt;Exam Review Note: CS5486 (First 220 Pages)&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;heading-1-support-vector-machines-svm--core-concepts&quot;&gt;1. &lt;strong&gt;Support Vector Machines (SVM) ‚Äì Core Concepts&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Maximal Margin Classifier&lt;/strong&gt; ‚Üí Better generalization, less overfitting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kernel Trick&lt;/strong&gt; ‚Üí Maps data to higher dimensions without explicit transformation (only dot products used).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kernel Selection &amp;amp; Parameters&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Domain experts can help design similarity measures.&lt;/li&gt;
      &lt;li&gt;Gaussian kernel: œÉ = distance between closest points of different classes.&lt;/li&gt;
      &lt;li&gt;If no reliable criterion ‚Üí use &lt;strong&gt;cross-validation&lt;/strong&gt; or &lt;strong&gt;validation set&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hard Margin vs. Soft Margin&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Hard margin: strict separation ‚Üí only works if data is perfectly separable.&lt;/li&gt;
      &lt;li&gt;Soft margin: allows some misclassification ‚Üí more practical, especially with noisy data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;heading-2-support-vector-regression-svr--least-squares-svm&quot;&gt;2. &lt;strong&gt;Support Vector Regression (SVR) &amp;amp; Least-Squares SVM&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SVR: Extension of SVM for regression.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Least-Squares SVM (LS-SVM)&lt;/strong&gt; by Suykens &amp;amp; Vandewalle (1999):
    &lt;ul&gt;
      &lt;li&gt;Primal problem ‚Üí Lagrangian ‚Üí Final model with closed-form solution.&lt;/li&gt;
      &lt;li&gt;Often faster than traditional SVM for regression.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;heading-3-neural-network-basics&quot;&gt;3. &lt;strong&gt;Neural Network Basics&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;heading--perceptrons&quot;&gt;‚û§ Perceptrons&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Limitations&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Can only classify &lt;strong&gt;linearly separable&lt;/strong&gt; data.&lt;/li&gt;
      &lt;li&gt;Slow convergence in high dimensions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bipolar vs Unipolar&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Bipolar ([-1, 1]) better for algebraic structure and weight space representation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;heading--adaline&quot;&gt;‚û§ ADALINE&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Adaptive Linear Element (Widrow-Hoff, 1960s).&lt;/li&gt;
  &lt;li&gt;Trained via &lt;strong&gt;Delta Rule / LMS Algorithm&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Learning rule: updates weights to minimize error (gradient descent).&lt;/li&gt;
  &lt;li&gt;Training modes: Sequential, Batch, or Perceptron-style.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;heading--maxnet&quot;&gt;‚û§ MAXNET&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Winner-Takes-All (WTA) architecture.&lt;/li&gt;
  &lt;li&gt;Uses self-excitatory &amp;amp; lateral-inhibitory connections.&lt;/li&gt;
  &lt;li&gt;Often used as output layer to select max input ‚Üí &quot;winner takes all&quot;.&lt;/li&gt;
  &lt;li&gt;Proven to be &lt;strong&gt;globally stable and convergent&lt;/strong&gt; (Julian kWTA model).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;heading-4-optimization--global-search&quot;&gt;4. &lt;strong&gt;Optimization &amp;amp; Global Search&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;heading--global-optimization&quot;&gt;‚û§ Global Optimization&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Search space: defined by min/max bounds of variables.&lt;/li&gt;
  &lt;li&gt;Schwefel‚Äôs function: benchmark for optimization (non-convex, multimodal).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evolutionary Algorithms (e.g., PSO, GA)&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Swarm evolution shown over iterations (0 ‚Üí 500).&lt;/li&gt;
      &lt;li&gt;Converged to global optimum: ~837.9658 (example value).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization is critical for hyperparameter tuning (e.g., œÉ, C, kernel choice).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;heading-5-logic--learning-theory&quot;&gt;5. &lt;strong&gt;Logic &amp;amp; Learning Theory&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Two-Variable Logic Functions&lt;/strong&gt; (OR, AND, Majority, XOR).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;XOR Problem&lt;/strong&gt;: Classic non-linearly separable problem ‚Üí &lt;strong&gt;cannot be solved by single-layer perceptron&lt;/strong&gt; ‚Üí needs multi-layer networks (e.g., MLP).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Monte Carlo Tests&lt;/strong&gt;: Used to evaluate robustness of learning algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;heading--key-takeaways-for-exam&quot;&gt;üß† &lt;strong&gt;Key Takeaways for Exam&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;‚úîÔ∏è Understand the &lt;strong&gt;difference between hard/soft margin SVMs&lt;/strong&gt;.&lt;br /&gt;
‚úîÔ∏è Know how to &lt;strong&gt;choose kernel parameters&lt;/strong&gt; (œÉ, etc.) and when to use cross-validation.&lt;br /&gt;
‚úîÔ∏è Be able to &lt;strong&gt;compare Perceptron, ADALINE, and MAXNET&lt;/strong&gt; ‚Äî strengths, weaknesses, use cases.&lt;br /&gt;
‚úîÔ∏è Understand the &lt;strong&gt;math behind LMS and gradient descent&lt;/strong&gt; for ADALINE.&lt;br /&gt;
‚úîÔ∏è Know why &lt;strong&gt;SVMs can handle non-linear data&lt;/strong&gt; via kernel trick.&lt;br /&gt;
‚úîÔ∏è Be familiar with &lt;strong&gt;optimization landscapes&lt;/strong&gt; (Schwefel‚Äôs function, evolutionary search).&lt;br /&gt;
‚úîÔ∏è Know &lt;strong&gt;why XOR can‚Äôt be solved by single-layer perceptrons&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;heading--recommended-focus-areas&quot;&gt;üìö &lt;strong&gt;Recommended Focus Areas&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;SVMs (Primal, Dual, Kernels, Hard vs Soft)&lt;/li&gt;
  &lt;li&gt;Neural Networks (Perceptron, ADALINE, MAXNET)&lt;/li&gt;
  &lt;li&gt;Optimization (Gradient descent, global search)&lt;/li&gt;
  &lt;li&gt;Logic functions (XOR, Majority, OR/AND)&lt;/li&gt;
  &lt;li&gt;Learning Algorithms (LMS, Delta Rule)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;‚úÖ &lt;strong&gt;Good luck on your exam!&lt;/strong&gt;&lt;br /&gt;
You‚Äôve got this ‚Äî especially if you‚Äôve reviewed the slides up to page 220. Focus on the &lt;em&gt;why&lt;/em&gt; behind the algorithms, not just the math.&lt;/p&gt;

&lt;hr /&gt;</content><author><name>Rain</name></author><category term="CityU" /><category term="Intelligent System" /><category term="Artificial Intelligence" /><category term="Neural Network" /><summary type="html">Intelligent System - Notes and Practice</summary></entry><entry><title type="html">Weird Python</title><link href="http://localhost:4000/blog/2025/10/19/WeirdPython/" rel="alternate" type="text/html" title="Weird Python" /><published>2025-10-19T00:00:00+08:00</published><updated>2025-10-19T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/19/WeirdPython</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/19/WeirdPython/">&lt;h1 id=&quot;heading-weird-python&quot;&gt;Weird Python&lt;/h1&gt;

&lt;h2 id=&quot;heading-introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I create this article because of the annoying confusing problems I met in the Python code. I have to admit that Python is a awesome programming language however‚Ä¶ it DOSE exist some weird problems when coding not carefully. So this passage is used to record the problems and solutions for Python debugging.&lt;/p&gt;

&lt;h2 id=&quot;heading-stable-method-for-patching-functions-in-packages&quot;&gt;Stable Method for Patching Functions in Packages&lt;/h2&gt;

&lt;h3 id=&quot;heading-problem&quot;&gt;Problem&lt;/h3&gt;

&lt;p&gt;Well, here&apos;s the problem. It happened when I was completing my tutorials. The tutorial imported a Python package called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lucent&lt;/code&gt;, which is used to visualize the layers of CNNs. The code was like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;lucent.optvis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;lucent.modelzoo.util&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It seems normal, but it may run into trouble because I will run this code on Jupyter Notebook (local or online). Then I use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lucent&lt;/code&gt; package to visualize the layers.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ldevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_model_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model1_conv_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;conv5:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;conv5:15&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:31&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:63&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:127&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_model_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model1_conv_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then the output cell run into trouble. There should be a beautiful progress bar loading in the output cell showing the current progress of the analyzation, however, the progress bar flashed and disappeared suddenly, leaving some static time text in the output cell. Like the cell below.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5it/s

second

second

second
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The situation is not what we want.&lt;/p&gt;

&lt;h3 id=&quot;heading-attempts--solution&quot;&gt;Attempts &amp;amp; Solution&lt;/h3&gt;

&lt;h3 id=&quot;heading-summary&quot;&gt;Summary&lt;/h3&gt;

&lt;h2 id=&quot;heading-„Å§„Å•„Åè&quot;&gt;„Å§„Å•„Åè‚Ä¶&lt;/h2&gt;</content><author><name>Rain</name></author><category term="Python" /><category term="Coding" /><category term="Debug" /><summary type="html">Weird Python</summary></entry><entry><title type="html">Quantitative Trading Notes</title><link href="http://localhost:4000/blog/2025/10/18/QuantTrading/" rel="alternate" type="text/html" title="Quantitative Trading Notes" /><published>2025-10-18T00:00:00+08:00</published><updated>2025-10-18T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/18/QuantTrading</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/18/QuantTrading/">&lt;h1 id=&quot;heading-quantitative-trading-notes&quot;&gt;Quantitative Trading Notes&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚ö†Ô∏è Warning: This article &lt;strong&gt;&lt;em&gt;DO NOT&lt;/em&gt;&lt;/strong&gt; provide any investiment advise. There are &lt;strong&gt;&lt;em&gt;RISKS&lt;/em&gt;&lt;/strong&gt; in investment and financial management, and you should be cautious.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;heading-„Å§„Å•„Åè&quot;&gt;„Å§„Å•„Åè‚Ä¶&lt;/h2&gt;</content><author><name>Rain</name></author><category term="Quantitative Trading" /><category term="FinTech" /><summary type="html">Quantitative Trading Notes</summary></entry><entry><title type="html">Explainable AI - Notes and Practice</title><link href="http://localhost:4000/blog/2025/10/17/XAI/" rel="alternate" type="text/html" title="Explainable AI - Notes and Practice" /><published>2025-10-17T00:00:00+08:00</published><updated>2025-10-17T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/17/XAI</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/17/XAI/">&lt;h1 id=&quot;heading-explainable-ai---notes-and-practice&quot;&gt;Explainable AI - Notes and Practice&lt;/h1&gt;

&lt;h2 id=&quot;heading-linear-regression-and-logistic-regression&quot;&gt;Linear Regression and Logistic Regression&lt;/h2&gt;

&lt;p&gt;We make regression because we want to make a &quot;supervise learning&quot;, that is, input observation $\mathbf{x}$ which is typically a vector in $\mathbb{R}^d$ and output a $y \in \mathbb{R}$ which is a real number. Our goal is to predict the output $y$ from input $\mathbf{x}$. That is, learn the function:&lt;/p&gt;

\[y = f(\mathbf{x}) + \epsilon\]

&lt;p&gt;where $\epsilon$ is the noise.&lt;/p&gt;

&lt;p&gt;Then we come to the linear regression.&lt;/p&gt;

&lt;h3 id=&quot;heading-linear-regression&quot;&gt;Linear Regression&lt;/h3&gt;

&lt;p&gt;Given a linear function of the input feature $x$, $f(x) = \omega * x + b$, where $\omega$ is the slope and $b$ is the intercept. Then we observe a noisy output of the function, then $f(x) = \omega * x + b + \epsilon$ where $\epsilon$ is a assumed Gaussian noise.&lt;/p&gt;

&lt;p&gt;We make it to the d-dimension case. The linear combination (also called weighted sum) of $d$ input variables $x_1, ‚Ä¶, x_d$. Now we have:&lt;/p&gt;

\[f(x) = \omega_0 + \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d\]

&lt;p&gt;where $x_j$ is the input features, $\omega_j$ is the input feature weights and $\omega_0$ is the intercept.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;KEY ASSUMPTIONS&lt;/strong&gt; of Linear Regression:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: Prediction is a linear combination of features. (Obviously, linear effects are easily interpretable; and additivity seperates the effects.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Normality&lt;/strong&gt;: Errors are normally distributed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Homoscedasticity&lt;/strong&gt;: Constant variance of errors across feature space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: Each instance is independent. (We do not measure a same thing repeatly.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fixed Features&lt;/strong&gt;: Features are treated as constants, free of measurement error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No Multicollinearity&lt;/strong&gt;: Features should not be strongly correlated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-ordinary-least-squares-ols&quot;&gt;Ordinary Least Squares (OLS)&lt;/h3&gt;

&lt;p&gt;Suppose we have a linear function:&lt;/p&gt;

\[f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b\]

&lt;p&gt;need to estimate the parameter $(\mathbf{w}, b)$ from the data. A method is to fit the parameters by minimizing the squared prediction error on the training set ${\lbrace(\mathbf{x}&lt;em&gt;i, y_i)\rbrace}^N&lt;/em&gt;{i=1}$. Like this:&lt;/p&gt;

\[\min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - f(x_i))^2}} = \min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - (\mathbf{w}^T \mathbf{x}_i + b))^2}}\]

&lt;p&gt;We have a &lt;strong&gt;closed-form solution&lt;/strong&gt; for this. The bias $b$ can be absorbed into $\mathbf{w}$ by redefining:&lt;/p&gt;

\[\mathbf{w} \leftarrow \begin{bmatrix}\mathbf{w}\\b\end{bmatrix}, \mathbf{x} \leftarrow \begin{bmatrix}\mathbf{x}\\1\end{bmatrix}\]

&lt;p&gt;then the minimization problem will be like this:&lt;/p&gt;

\[\min_{\mathbf{w}} ||\mathbf{y} - \mathbf{X}^T \mathbf{w}||^2\]

&lt;p&gt;where $\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, ‚Ä¶, \mathbf{x_N}]$ is the data matrix and $\mathbf{y} = [y_1, y_2, ‚Ä¶, y_N]^T$ is the vector of outputs.&lt;/p&gt;

&lt;p&gt;So we get the closed-form solution.&lt;/p&gt;

\[\mathbf{w}^* = (\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}\mathbf{y}\]

&lt;p&gt;The $(\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}$ part is also called the pseudo-inverse of $\mathbf{X}$.&lt;/p&gt;

&lt;h3 id=&quot;heading-model-fit-r-square&quot;&gt;Model Fit-R-Square&lt;/h3&gt;

&lt;p&gt;Commonly known as &lt;strong&gt;R-Squared&lt;/strong&gt; ($R^2$), is a statistical measure that shows the proportion of the variance in the dependent variable that can be explained by the independent variables in a regression model. It essentially indicates how well the regression model fits the observed data.&lt;/p&gt;

&lt;p&gt;For the prediction: $\hat{y_i} = f(x_i)$, R-Squared (Coefficient of Determination): proportion of variance is explained by the model.&lt;/p&gt;

\[R^2 = 1 - \frac{SSE}{SST}\]

&lt;p&gt;where the $SSE$ is the Sum of Squared Errors, calculated by $SSE = \sum^{n}&lt;em&gt;{i = 1}(y_i - \hat{y_i})^2$, measuring the total squared difference between actual and predicted values; and the $SST$ is the Total Sum of Squares, $SST = \sum^{n}&lt;/em&gt;{i = 1}(y_i - \bar{y})2$, measuring the total variance in the observed data, and $\bar{y}$ is the mean.&lt;/p&gt;

&lt;p&gt;Now we get $R^2$ can we can make an interpretation. $R^2$ closed to 1 means model explains most of the variance; and $R^2$ closed to 0 means model explains little of the variance.&lt;/p&gt;

&lt;p&gt;Then we have Adjusted R-Squared:&lt;/p&gt;

\[\bar{R}^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}\]

&lt;p&gt;and it adjusts $R^2$ for the number of predictors $p$ and the sample size $n$. Thus, we can use Adjusted R-Squared when comparing models with different numbers of features. Also, Adjusted R-Squared penalizes unnecessary predictors.&lt;/p&gt;

&lt;h2 id=&quot;heading-„Å§„Å•„Åè&quot;&gt;„Å§„Å•„Åè‚Ä¶&lt;/h2&gt;</content><author><name>Rain</name></author><category term="CityU" /><category term="Artificial Intelligence" /><category term="XAI" /><summary type="html">Explainable AI - Notes and Practice</summary></entry></feed>