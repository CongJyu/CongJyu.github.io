<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-22T18:12:12+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rain Chen&apos;s Blog</title><subtitle>One Personal Site.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;job_title&quot;=&gt;nil, &quot;location&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;social_links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/congjyu&quot;}, {&quot;name&quot;=&gt;&quot;rss&quot;, &quot;url&quot;=&gt;&quot;https://congjyu.github.io/feed&quot;}]}</name></author><entry><title type="html">Paper Reading - Open-Sora 2.0</title><link href="http://localhost:4000/blog/2025/11/14/Paper-OpenSora/" rel="alternate" type="text/html" title="Paper Reading - Open-Sora 2.0" /><published>2025-11-14T00:00:00+08:00</published><updated>2025-11-14T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/11/14/Paper-OpenSora</id><content type="html" xml:base="http://localhost:4000/blog/2025/11/14/Paper-OpenSora/">&lt;h1 id=&quot;heading-paper-reading---open-sora-20-training-a-commercial-level-video-generation-model-in-200k&quot;&gt;Paper Reading - Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k&lt;/h1&gt;

&lt;p&gt;This paper is written by Open-Sora Team.&lt;/p&gt;

&lt;h2 id=&quot;heading-motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Video generation models have achieved remarkable progress in the pass year and the quality of the generated videos are
also improved. But the costs and data quantity of these models increased, and it demands a greater training compute. The
author presented a commercial-level video generation model that only costs $200k.&lt;/p&gt;

&lt;p&gt;To evaluate the Open-Sora model, the team has designed a comparing table, which covers three key aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visual quality&lt;/li&gt;
  &lt;li&gt;Prompt adherence&lt;/li&gt;
  &lt;li&gt;Motion quanlity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;heading-model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;There are two key components of a video generation model: the autoencoder and the diffusion transformer. For the
autoencoder, Open-Sora is initially trained on HunyuanVideo&apos;s VAE and later adapt to the Video DC-AE.&lt;/p&gt;

&lt;h3 id=&quot;heading-3d-autoencoder&quot;&gt;3D Autoencoder&lt;/h3&gt;

&lt;p&gt;While training this model, an open source video generation model HunyunVideo VAE is leveraged, and the team developed a
video autoencoder with deep compression to improve efficiency and decrease the costs while maintaining high
reconstruction fidelity, which is called &quot;Video DC-AE&quot;.&lt;/p&gt;

&lt;p&gt;The overview of Video DC-AE: Each block in encoder introduces spatial downsampling, while temporal downsampling occurs
at blocks 4 and 5, with a corresponding symmetric structure in the decoder. The Video DC-AE encoder consists of three
residual blocks followed by three EfficientViT blocks, with the decoder adopting a symmetrical structure.&lt;/p&gt;

&lt;h2 id=&quot;heading-why-the-3-main-stages-in-training-process-structured-this-way&quot;&gt;Why the 3 main stages in training process structured this way?&lt;/h2&gt;

&lt;p&gt;Open-Sora 2.0 uses a &lt;strong&gt;three-stage training pipeline&lt;/strong&gt; to achieve commercial-level video generation quality (comparable
to models like HunyuanVideo and Runway Gen-3 Alpha) at dramatically lower cost (~$200k total, 5–10× cheaper than
comparable models). The structure prioritizes efficiency by doing the heavy lifting (learning diverse motion patterns
and semantics) at low resolution with cheaper compute, leveraging strong pre-trained image models, and reserving
expensive high-resolution training for a short final fine-tuning phase.&lt;/p&gt;

&lt;p&gt;This avoids the prohibitive costs of training directly on high-resolution data from scratch, where attention complexity
scales quadratically with token count/resolution, and minimizes token processing via a custom Video Deep Compression
Autoencoder (Video DC-AE).&lt;/p&gt;

&lt;h3 id=&quot;heading-stage-1-low-resolution-text-to-video-t2v-pretraining-256px&quot;&gt;Stage 1: Low-Resolution Text-to-Video (T2V) Pretraining (256px)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;What happens&lt;/strong&gt;: The model (11B parameters, Diffusion Transformer-based) initializes from Flux (a strong open-source
text-to-image model). It trains as text-to-video on ~70M carefully curated 256px short video clips for 85k iterations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: ~$107.5k (most of the budget).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why this stage first&lt;/strong&gt;: Low resolution keeps token counts and attention costs very low, allowing the model to
efficiently learn diverse real-world motion patterns and text-video alignment on a massive scale. Starting from a
pretrained T2I model (Flux) massively accelerates convergence instead of training video dynamics from scratch. The
paper states: &quot;we first train on 256px resolution videos, allowing the model to learn diverse motion patterns
efficiently.&quot;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-stage-2-low-resolution-image-to-video-i2v-adaptation-256px&quot;&gt;Stage 2: Low-Resolution Image-to-Video (I2V) Adaptation (256px)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;What happens&lt;/strong&gt;: Continues from Stage 1 checkpoint, shifts to image-to-video conditioning (first frame provided as
input), training on a higher-quality subset of ~10M 256px clips for 13k iterations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: ~$18.4k.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why switch to I2V here&lt;/strong&gt;: Image conditioning lets the model reuse powerful pretrained image features (especially
from Flux) and focus purely on temporal/motion modeling. This sets up much more efficient resolution upscaling later.
The authors note that &quot;adapting a model from 256px to 768px resolution is significantly more efficient using an
image-to-video approach&quot; because I2V avoids regenerating the first frame from text at high res (which is harder/less
stable) and instead concentrates compute on coherent motion given a sharp starting image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-stage-3-high-resolution-image-to-video-fine-tuning-768px&quot;&gt;Stage 3: High-Resolution Image-to-Video Fine-Tuning (768px)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;What happens&lt;/strong&gt;: Fine-tunes the Stage 2 checkpoint as I2V (with text+image conditioning supported in the final model)
on ~5M strictly filtered high-quality 768px videos for 13k iterations, using the Video DC-AE for extreme compression (
4× temporal × 32× spatial = 4×32×32 overall, reducing tokens ~16× vs standard VAEs).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: ~$73.7k.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why only fine-tune at high res, and why I2V&lt;/strong&gt;: High resolution is computationally brutal (much higher token counts),
so they limit it to a short, targeted phase with a smaller dataset. Motion priors already learned in low-res stages
transfer well, and I2V focuses effort on refining dynamics/perceptual quality rather than re-learning static
composition. The paper emphasizes: &quot;increasing the resolution significantly improves perceptual quality,&quot; but doing it
early would explode costs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-overall-rationale-for-the-3-stage-structure&quot;&gt;Overall Rationale for the 3-Stage Structure&lt;/h3&gt;

&lt;p&gt;The core insight is progressive/curricular training: learn motion cheaply at low res → adapt to stronger image
conditioning → polish at high res only. This + Flux initialization + Video DC-AE compression gives massive savings while
preserving quality stays on par with models trained for millions of dollars. Direct high-res T2V training from scratch,
or doing everything in one stage, would require far more GPU time because of quadratic scaling and poorer convergence.
By decoupling motion learning (low-res) from visual fidelity (high-res fine-tune) and using I2V for the expensive parts,
they achieve 5.2× training throughput gains and &amp;gt;10× inference speedup versus less-compressed alternatives.&lt;/p&gt;

&lt;p&gt;In short, the staging is an elegant cost–quality optimization that proves you don&apos;t need million-dollar runs to reach
the frontier — you just need to train smart.&lt;/p&gt;</content><author><name>Rain</name></author><category term="Artificial Intelligence" /><category term="Papers" /><summary type="html">Paper Reading - Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k</summary></entry><entry><title type="html">Intelligent System - Notes and Practice</title><link href="http://localhost:4000/blog/2025/10/30/IntelligentSystem/" rel="alternate" type="text/html" title="Intelligent System - Notes and Practice" /><published>2025-10-30T00:00:00+08:00</published><updated>2025-10-30T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/30/IntelligentSystem</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/30/IntelligentSystem/">&lt;h1 id=&quot;heading-intelligent-system---notes-and-practice&quot;&gt;Intelligent System - Notes and Practice&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Intelligence is a mental quality that consists of the abilities to learn from experience, adapt to new situations,
understanding and handle abstract concepts, and use knowledge to manipulate one&apos;s environment.&lt;/p&gt;

  &lt;p&gt;– Britannica&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;heading-a-definition-of-intelligent-systems&quot;&gt;A Definition of Intelligent Systems&lt;/h3&gt;

&lt;p&gt;A system is an intelligent system if the system exhibits some intelligent behaviors. (e.g. fuzzy systems, simulated
annealing, genetic algorithms and expert systems.)&lt;/p&gt;

&lt;h2 id=&quot;heading-support-vector-machines-svm&quot;&gt;Support Vector Machines (SVM)&lt;/h2&gt;

&lt;h2 id=&quot;heading-つづく&quot;&gt;つづく…&lt;/h2&gt;</content><author><name>Rain</name></author><category term="CityU" /><category term="Intelligent System" /><category term="Artificial Intelligence" /><category term="Neural Network" /><summary type="html">Intelligent System - Notes and Practice</summary></entry><entry><title type="html">Weird Python</title><link href="http://localhost:4000/blog/2025/10/19/WeirdPython/" rel="alternate" type="text/html" title="Weird Python" /><published>2025-10-19T00:00:00+08:00</published><updated>2025-10-19T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/19/WeirdPython</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/19/WeirdPython/">&lt;h1 id=&quot;heading-weird-python&quot;&gt;Weird Python&lt;/h1&gt;

&lt;h2 id=&quot;heading-introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I create this article because of the annoying confusing problems I met in the Python code. I have to admit that Python
is a awesome programming language however… it DOSE exist some weird problems when coding not carefully. So this
passage is used to record the problems and solutions for Python debugging.&lt;/p&gt;

&lt;h2 id=&quot;heading-stable-method-for-patching-functions-in-packages&quot;&gt;Stable Method for Patching Functions in Packages&lt;/h2&gt;

&lt;h3 id=&quot;heading-problem&quot;&gt;Problem&lt;/h3&gt;

&lt;p&gt;Well, here&apos;s the problem. It happened when I was completing my tutorials. The tutorial imported a Python package called
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lucent&lt;/code&gt;, which is used to visualize the layers of CNNs. The code was like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;lucent.optvis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;lucent.modelzoo.util&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It seems normal, but it may run into trouble because I will run this code on Jupyter Notebook (local or online). Then I
use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lucent&lt;/code&gt; package to visualize the layers.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ldevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_model_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model1_conv_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;conv5:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;conv5:15&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:31&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:63&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5:127&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lu_zoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_model_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model1_conv_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_vis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model1_conv_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then the output cell run into trouble. There should be a beautiful progress bar loading in the output cell showing the
current progress of the analyzation, however, the progress bar flashed and disappeared suddenly, leaving some static
time text in the output cell. Like the cell below.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5it/s

second

second

second
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The situation is not what we want.&lt;/p&gt;

&lt;h3 id=&quot;heading-attempts--solution&quot;&gt;Attempts &amp;amp; Solution&lt;/h3&gt;

&lt;p&gt;The solution is simple too. Now the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; package provides a special module for Jupyter Notebook, so we can use that
module to show progress bar in Jupyter Notebook environment. However, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; package is not import directly and it
is import by another package. How can we replace the original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; method to the new specified Jupyter Notebook &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm&lt;/code&gt;
method?&lt;/p&gt;

&lt;p&gt;Change the code file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lucent&lt;/code&gt; package and make it use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;notebook.tqdm&lt;/code&gt; directly DOES work… but it seems that it
is not that &quot;normal&quot;. (i.e. totally a strange method, does not robust and stable at all). So, any other methods?&lt;/p&gt;

&lt;p&gt;Luckily, there is an elegant way to solve this problem. We import &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sys&lt;/code&gt; the system package first. Then we import the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm.notebook&lt;/code&gt; package. Finally we can &quot;patch&quot; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tqdm.notebook.tqdm&lt;/code&gt;, and this will help us enable the
Jupyter Notebook progress bar.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;notebook&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tqdm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;notebook&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Re-run the Jupyter Notebook, and we find that everything goes well.&lt;/p&gt;

&lt;p&gt;Also, this method works well with other situations when some packages need to be change in certain imported packages.&lt;/p&gt;

&lt;h2 id=&quot;heading-つづく&quot;&gt;つづく…&lt;/h2&gt;</content><author><name>Rain</name></author><category term="Python" /><category term="Coding" /><category term="Debug" /><summary type="html">Weird Python</summary></entry><entry><title type="html">Quantitative Trading Notes</title><link href="http://localhost:4000/blog/2025/10/18/QuantTrading/" rel="alternate" type="text/html" title="Quantitative Trading Notes" /><published>2025-10-18T00:00:00+08:00</published><updated>2025-10-18T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/18/QuantTrading</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/18/QuantTrading/">&lt;h1 id=&quot;heading-quantitative-trading-notes&quot;&gt;Quantitative Trading Notes&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ Warning: This article &lt;strong&gt;&lt;em&gt;DO NOT&lt;/em&gt;&lt;/strong&gt; provide any investiment advise. There are &lt;strong&gt;&lt;em&gt;RISKS&lt;/em&gt;&lt;/strong&gt; in investment and
financial management, and you should be cautious.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;heading-つづく&quot;&gt;つづく…&lt;/h2&gt;</content><author><name>Rain</name></author><category term="Quantitative Trading" /><category term="FinTech" /><summary type="html">Quantitative Trading Notes</summary></entry><entry><title type="html">Explainable AI - Notes and Practice</title><link href="http://localhost:4000/blog/2025/10/17/XAI/" rel="alternate" type="text/html" title="Explainable AI - Notes and Practice" /><published>2025-10-17T00:00:00+08:00</published><updated>2025-10-17T00:00:00+08:00</updated><id>http://localhost:4000/blog/2025/10/17/XAI</id><content type="html" xml:base="http://localhost:4000/blog/2025/10/17/XAI/">&lt;h1 id=&quot;heading-explainable-ai---notes-and-practice&quot;&gt;Explainable AI - Notes and Practice&lt;/h1&gt;

&lt;h2 id=&quot;heading-linear-regression-and-logistic-regression&quot;&gt;Linear Regression and Logistic Regression&lt;/h2&gt;

&lt;p&gt;We make regression because we want to make a &quot;supervise learning&quot;, that is, input observation $\mathbf{x}$ which is
typically a vector in $\mathbb{R}^d$ and output a $y \in \mathbb{R}$ which is a real number. Our goal is to predict the
output $y$ from input $\mathbf{x}$. That is, learn the function:&lt;/p&gt;

\[y = f(\mathbf{x}) + \epsilon\]

&lt;p&gt;where $\epsilon$ is the noise.&lt;/p&gt;

&lt;p&gt;Then we come to the linear regression.&lt;/p&gt;

&lt;h3 id=&quot;heading-linear-regression&quot;&gt;Linear Regression&lt;/h3&gt;

&lt;p&gt;Given a linear function of the input feature $x$, $f(x) = \omega * x + b$, where $\omega$ is the slope and $b$ is the
intercept. Then we observe a noisy output of the function, then $f(x) = \omega * x + b + \epsilon$ where $\epsilon$ is a
assumed Gaussian noise.&lt;/p&gt;

&lt;p&gt;We make it to the d-dimension case. The linear combination (also called weighted sum) of $d$ input
variables $x_1, …, x_d$. Now we have:&lt;/p&gt;

\[f(x) = \omega_0 + \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d\]

&lt;p&gt;where $x_j$ is the input features, $\omega_j$ is the input feature weights and $\omega_0$ is the intercept.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;KEY ASSUMPTIONS&lt;/strong&gt; of Linear Regression:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linearity&lt;/strong&gt;: Prediction is a linear combination of features. (Obviously, linear effects are easily interpretable;
and additivity seperates the effects.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Normality&lt;/strong&gt;: Errors are normally distributed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Homoscedasticity&lt;/strong&gt;: Constant variance of errors across feature space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Independence&lt;/strong&gt;: Each instance is independent. (We do not measure a same thing repeatly.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fixed Features&lt;/strong&gt;: Features are treated as constants, free of measurement error.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No Multicollinearity&lt;/strong&gt;: Features should not be strongly correlated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heading-ordinary-least-squares-ols&quot;&gt;Ordinary Least Squares (OLS)&lt;/h3&gt;

&lt;p&gt;Suppose we have a linear function:&lt;/p&gt;

\[f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b\]

&lt;p&gt;need to estimate the parameter $(\mathbf{w}, b)$ from the data. A method is to fit the parameters by minimizing the
squared prediction error on the training set&lt;/p&gt;

\[{\lbrace(\mathbf{x}_i, y_i)\rbrace}^{N}_{i = 1}\]

&lt;p&gt;Like this:&lt;/p&gt;

\[\min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - f(x_i))^2}} = \min_{\mathbf{w}, b}{\sum^{N}_{i = 1}{(y_i - (\mathbf{w}^T \mathbf{x}_i + b))^2}}\]

&lt;p&gt;We have a &lt;strong&gt;closed-form solution&lt;/strong&gt; for this. The bias $b$ can be absorbed into $\mathbf{w}$ by redefining:&lt;/p&gt;

\[\mathbf{w} \leftarrow \begin{bmatrix}\mathbf{w}\\b\end{bmatrix}, \mathbf{x} \leftarrow \begin{bmatrix}\mathbf{x}\\1\end{bmatrix}\]

&lt;p&gt;then the minimization problem will be like this:&lt;/p&gt;

\[\min_{\mathbf{w}} ||\mathbf{y} - \mathbf{X}^T \mathbf{w}||^2\]

&lt;p&gt;where $\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, …, \mathbf{x_N}]$ is the data matrix
and $\mathbf{y} = [y_1, y_2, …, y_N]^T$ is the vector of outputs.&lt;/p&gt;

&lt;p&gt;So we get the closed-form solution.&lt;/p&gt;

\[\mathbf{w}^* = (\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}\mathbf{y}\]

&lt;p&gt;The $(\mathbf{X}\mathbf{X}^T)^{-1}\mathbf{X}$ part is also called the pseudo-inverse of $\mathbf{X}$.&lt;/p&gt;

&lt;h3 id=&quot;heading-model-fit-r-square&quot;&gt;Model Fit-R-Square&lt;/h3&gt;

&lt;p&gt;Commonly known as &lt;strong&gt;R-Squared&lt;/strong&gt; ($R^2$), is a statistical measure that shows the proportion of the variance in the
dependent variable that can be explained by the independent variables in a regression model. It essentially indicates
how well the regression model fits the observed data.&lt;/p&gt;

&lt;p&gt;For the prediction: $\hat{y_i} = f(x_i)$, R-Squared (Coefficient of Determination): proportion of variance is explained
by the model.&lt;/p&gt;

\[R^2 = 1 - \frac{SSE}{SST}\]

&lt;p&gt;where the $SSE$ is the Sum of Squared Errors, calculated by&lt;/p&gt;

\[SSE = \sum^{n}_{i = 1}(y_i - \hat{y_i})^2\]

&lt;p&gt;measuring the total squared difference between actual and predicted values; and the $SST$ is the Total Sum of
Squares, $SST = \sum^{n}_{i = 1}(y_i - \bar{y})^2$, measuring the total variance in the observed data, and $\bar{y}$ is
the mean.&lt;/p&gt;

&lt;p&gt;Now we get $R^2$ can we can make an interpretation. $R^2$ closed to 1 means model explains most of the variance;
and $R^2$ closed to 0 means model explains little of the variance.&lt;/p&gt;

&lt;p&gt;Then we have Adjusted R-Squared:&lt;/p&gt;

\[\bar{R}^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}\]

&lt;p&gt;and it adjusts $R^2$ for the number of predictors $p$ and the sample size $n$. Thus, we can use Adjusted R-Squared when
comparing models with different numbers of features. Also, Adjusted R-Squared penalizes unnecessary predictors.&lt;/p&gt;

&lt;h2 id=&quot;heading-つづく&quot;&gt;つづく…&lt;/h2&gt;</content><author><name>Rain</name></author><category term="CityU" /><category term="Artificial Intelligence" /><category term="XAI" /><summary type="html">Explainable AI - Notes and Practice</summary></entry></feed>